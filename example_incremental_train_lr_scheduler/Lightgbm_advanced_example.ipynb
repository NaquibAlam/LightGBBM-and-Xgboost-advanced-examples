{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m0a04ut/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except BaseException:\n",
    "    import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mData\u001b[m\u001b[m                            Lightgbm_advanced_example.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "# load or create your dataset\n",
    "df_train = pd.read_csv('https://raw.githubusercontent.com/microsoft/LightGBM/master/examples/binary_classification/binary.train', header=None, sep='\\t')\n",
    "df_test = pd.read_csv('https://raw.githubusercontent.com/microsoft/LightGBM/master/examples/binary_classification/binary.test', header=None, sep='\\t')\n",
    "W_train = pd.read_csv('Data/binary.train.weight', header=None)[0]\n",
    "W_test = pd.read_csv('Data/binary.test.weight', header=None)[0]\n",
    "\n",
    "y_train = df_train[0]\n",
    "y_test = df_test[0]\n",
    "X_train = df_train.drop(0, axis=1)\n",
    "X_test = df_test.drop(0, axis=1)\n",
    "\n",
    "num_train, num_feature = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 29) (500, 29) (7000,) (500,)\n",
      "   0      1      2      3      4      5      6      7      8      9   ...  \\\n",
      "0   1  0.869 -0.635  0.226  0.327 -0.690  0.754 -0.249 -1.092  0.000  ...   \n",
      "1   1  0.908  0.329  0.359  1.498 -0.313  1.096 -0.558 -1.588  2.173  ...   \n",
      "2   1  0.799  1.471 -1.636  0.454  0.426  1.105  1.282  1.382  0.000  ...   \n",
      "3   0  1.344 -0.877  0.936  1.992  0.882  1.786 -1.647 -0.942  0.000  ...   \n",
      "4   1  1.105  0.321  1.522  0.883 -1.205  0.681 -1.070 -0.922  0.000  ...   \n",
      "\n",
      "      19     20     21     22     23     24     25     26     27     28  \n",
      "0 -0.010 -0.046  3.102  1.354  0.980  0.978  0.920  0.722  0.989  0.877  \n",
      "1 -1.139 -0.001  0.000  0.302  0.833  0.986  0.978  0.780  0.992  0.798  \n",
      "2  1.129  0.900  0.000  0.910  1.108  0.986  0.951  0.803  0.866  0.780  \n",
      "3 -0.678 -1.360  0.000  0.947  1.029  0.999  0.728  0.869  1.027  0.958  \n",
      "4 -0.374  0.113  0.000  0.756  1.361  0.987  0.838  1.133  0.872  0.808  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "   0      1      2      3      4      5      6      7      8      9   ...  \\\n",
      "0   1  0.644  0.247 -0.447  0.862  0.374  0.854 -1.126 -0.790  2.173  ...   \n",
      "1   0  0.385  1.800  1.037  1.044  0.349  1.502 -0.966  1.734  0.000  ...   \n",
      "2   0  1.214 -0.166  0.004  0.505  1.434  0.628 -1.174 -1.230  1.087  ...   \n",
      "3   1  0.420  1.111  0.137  1.516 -1.657  0.854  0.623  1.605  1.087  ...   \n",
      "4   0  0.897 -1.703 -1.306  1.022 -0.729  0.836  0.859 -0.333  2.173  ...   \n",
      "\n",
      "      19     20     21     22     23     24     25     26     27     28  \n",
      "0 -0.190 -0.744  3.102  0.958  1.061  0.980  0.875  0.581  0.905  0.796  \n",
      "1 -0.440  0.638  3.102  0.695  0.909  0.981  0.803  0.813  1.149  1.116  \n",
      "2 -1.383  1.355  0.000  0.848  0.911  1.043  0.931  1.058  0.744  0.696  \n",
      "3  0.731  1.424  3.102  1.597  1.282  1.105  0.730  0.148  1.231  1.234  \n",
      "4 -2.019 -0.289  0.000  0.805  0.930  0.984  1.430  2.198  1.934  1.684  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "0    1.2\n",
      "1    1.1\n",
      "2    1.0\n",
      "3    1.0\n",
      "4    1.0\n",
      "Name: 0, dtype: float64\n",
      "0    1.2\n",
      "1    1.1\n",
      "2    1.0\n",
      "3    1.0\n",
      "4    1.0\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape, df_test.shape, W_train.shape, W_test.shape)\n",
    "print(df_train.head())\n",
    "print(df_test.head())\n",
    "print(W_train.head())\n",
    "print(W_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "[1]\ttraining's binary_logloss: 0.680295\n",
      "[2]\ttraining's binary_logloss: 0.672016\n",
      "[3]\ttraining's binary_logloss: 0.664438\n",
      "[4]\ttraining's binary_logloss: 0.655529\n",
      "[5]\ttraining's binary_logloss: 0.647367\n",
      "[6]\ttraining's binary_logloss: 0.640943\n",
      "[7]\ttraining's binary_logloss: 0.635131\n",
      "[8]\ttraining's binary_logloss: 0.628759\n",
      "[9]\ttraining's binary_logloss: 0.622764\n",
      "[10]\ttraining's binary_logloss: 0.616886\n",
      "Finished first 10 rounds...\n",
      "7th feature name is: feature_6\n",
      "Saving model...\n",
      "Dumping model to JSON...\n",
      "Feature names: ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_12', 'feature_13', 'feature_14', 'feature_15', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_27']\n",
      "Feature importances: [9, 6, 1, 15, 5, 40, 3, 0, 0, 8, 2, 1, 0, 9, 2, 0, 0, 6, 2, 6, 0, 0, 37, 2, 30, 50, 37, 29]\n",
      "Loading model to predict...\n",
      "The rmse of loaded model's prediction is: 0.4624087630678351\n",
      "Dumping and loading model with pickle...\n",
      "The rmse of pickled model's prediction is: 0.46998810829655624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m0a04ut/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [21]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    }
   ],
   "source": [
    "# create dataset for lightgbm\n",
    "# if you want to re-use data, remember to set free_raw_data=False\n",
    "lgb_train = lgb.Dataset(X_train, y_train,\n",
    "                        weight=W_train, free_raw_data=False)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train,\n",
    "                       weight=W_test, free_raw_data=False)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# generate feature names\n",
    "feature_name = ['feature_' + str(col) for col in range(num_feature)]\n",
    "\n",
    "print('Starting training...')\n",
    "# feature_name and categorical_feature\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10,\n",
    "                valid_sets=lgb_train,  # eval training data\n",
    "                feature_name=feature_name,\n",
    "                categorical_feature=[21])\n",
    "\n",
    "print('Finished first 10 rounds...')\n",
    "# check feature name\n",
    "print('7th feature name is:', lgb_train.feature_name[6])\n",
    "\n",
    "print('Saving model...')\n",
    "# save model to file\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "print('Dumping model to JSON...')\n",
    "# dump model to JSON (and save to file)\n",
    "model_json = gbm.dump_model()\n",
    "\n",
    "with open('model.json', 'w+') as f:\n",
    "    json.dump(model_json, f, indent=4)\n",
    "\n",
    "# feature names\n",
    "print('Feature names:', gbm.feature_name())\n",
    "\n",
    "# feature importances\n",
    "print('Feature importances:', list(gbm.feature_importance()))\n",
    "\n",
    "print('Loading model to predict...')\n",
    "# load model to predict\n",
    "bst = lgb.Booster(model_file='model.txt')\n",
    "# can only predict with the best iteration (or the saving iteration)\n",
    "y_pred = bst.predict(X_test)\n",
    "# eval with loaded model\n",
    "print(\"The rmse of loaded model's prediction is:\", mean_squared_error(y_test, y_pred) ** 0.5)\n",
    "\n",
    "print('Dumping and loading model with pickle...')\n",
    "# dump model with pickle\n",
    "with open('model.pkl', 'wb') as fout:\n",
    "    pickle.dump(gbm, fout)\n",
    "# load model with pickle to predict\n",
    "with open('model.pkl', 'rb') as fin:\n",
    "    pkl_bst = pickle.load(fin)\n",
    "# can predict with any iteration when loaded in pickle way\n",
    "y_pred = pkl_bst.predict(X_test, num_iteration=7)\n",
    "# eval with loaded model\n",
    "print(\"The rmse of pickled model's prediction is:\", mean_squared_error(y_test, y_pred) ** 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\tvalid_0's binary_logloss: 0.614212\n",
      "[12]\tvalid_0's binary_logloss: 0.609782\n",
      "[13]\tvalid_0's binary_logloss: 0.605261\n",
      "[14]\tvalid_0's binary_logloss: 0.601549\n",
      "[15]\tvalid_0's binary_logloss: 0.598284\n",
      "[16]\tvalid_0's binary_logloss: 0.59599\n",
      "[17]\tvalid_0's binary_logloss: 0.591792\n",
      "[18]\tvalid_0's binary_logloss: 0.588347\n",
      "[19]\tvalid_0's binary_logloss: 0.585768\n",
      "[20]\tvalid_0's binary_logloss: 0.582936\n",
      "Finished 10 - 20 rounds with model file...\n",
      "[21]\tvalid_0's binary_logloss: 0.579478\n",
      "[22]\tvalid_0's binary_logloss: 0.57816\n",
      "[23]\tvalid_0's binary_logloss: 0.575105\n",
      "[24]\tvalid_0's binary_logloss: 0.572562\n",
      "[25]\tvalid_0's binary_logloss: 0.570356\n",
      "[26]\tvalid_0's binary_logloss: 0.569028\n",
      "[27]\tvalid_0's binary_logloss: 0.56685\n",
      "[28]\tvalid_0's binary_logloss: 0.565815\n",
      "[29]\tvalid_0's binary_logloss: 0.564377\n",
      "[30]\tvalid_0's binary_logloss: 0.562884\n",
      "Finished 20 - 30 rounds with decay learning rates...\n",
      "[31]\tvalid_0's binary_logloss: 0.560326\n",
      "[32]\tvalid_0's binary_logloss: 0.559062\n",
      "[33]\tvalid_0's binary_logloss: 0.556973\n",
      "[34]\tvalid_0's binary_logloss: 0.555429\n",
      "[35]\tvalid_0's binary_logloss: 0.554186\n",
      "[36]\tvalid_0's binary_logloss: 0.552226\n",
      "[37]\tvalid_0's binary_logloss: 0.550174\n",
      "[38]\tvalid_0's binary_logloss: 0.548788\n",
      "[39]\tvalid_0's binary_logloss: 0.547069\n",
      "[40]\tvalid_0's binary_logloss: 0.545801\n",
      "Finished 30 - 40 rounds with changing bagging_fraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/m0a04ut/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:1243: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/Users/m0a04ut/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py:1247: UserWarning: categorical_feature in Dataset is overridden.\n",
      "New categorical_feature is [21]\n",
      "  'New categorical_feature is {}'.format(sorted(list(categorical_feature))))\n"
     ]
    }
   ],
   "source": [
    "# continue training\n",
    "# init_model accepts:\n",
    "# 1. model file name\n",
    "# 2. Booster()\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10,\n",
    "                init_model='model.txt',\n",
    "                valid_sets=lgb_eval)\n",
    "\n",
    "print('Finished 10 - 20 rounds with model file...')\n",
    "\n",
    "# decay learning rates\n",
    "# learning_rates accepts:\n",
    "# 1. list/tuple with length = num_boost_round\n",
    "# 2. function(curr_iter)\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10,\n",
    "                init_model=gbm,\n",
    "                learning_rates=lambda iter: 0.05 * (0.99 ** iter),\n",
    "                valid_sets=lgb_eval)\n",
    "\n",
    "print('Finished 20 - 30 rounds with decay learning rates...')\n",
    "\n",
    "# change other parameters during training\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10,\n",
    "                init_model=gbm,\n",
    "                valid_sets=lgb_eval,\n",
    "                callbacks=[lgb.reset_parameter(bagging_fraction=[0.7] * 5 + [0.6] * 5)])\n",
    "\n",
    "print('Finished 30 - 40 rounds with changing bagging_fraction...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71]\tvalid_0's binary_logloss: 5.6909\tvalid_0's error: 0.262\n",
      "[72]\tvalid_0's binary_logloss: 5.62967\tvalid_0's error: 0.258\n",
      "[73]\tvalid_0's binary_logloss: 5.68806\tvalid_0's error: 0.258\n",
      "[74]\tvalid_0's binary_logloss: 5.68529\tvalid_0's error: 0.256\n",
      "[75]\tvalid_0's binary_logloss: 5.63089\tvalid_0's error: 0.256\n",
      "[76]\tvalid_0's binary_logloss: 5.6256\tvalid_0's error: 0.256\n",
      "[77]\tvalid_0's binary_logloss: 5.68221\tvalid_0's error: 0.256\n",
      "[78]\tvalid_0's binary_logloss: 5.67482\tvalid_0's error: 0.258\n",
      "[79]\tvalid_0's binary_logloss: 5.66856\tvalid_0's error: 0.258\n",
      "[80]\tvalid_0's binary_logloss: 5.60627\tvalid_0's error: 0.258\n",
      "Finished 40 - 50 rounds with self-defined objective function and eval metric...\n"
     ]
    }
   ],
   "source": [
    "# self-defined objective function\n",
    "# f(preds: array, train_data: Dataset) -> grad: array, hess: array\n",
    "# log likelihood loss\n",
    "def loglikelihood(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    preds = 1. / (1. + np.exp(-preds))\n",
    "    grad = preds - labels\n",
    "    hess = preds * (1. - preds)\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool\n",
    "# binary error\n",
    "# NOTE: when you do customized loss function, the default prediction value is margin\n",
    "# This may make built-in evalution metric calculate wrong results\n",
    "# For example, we are doing log likelihood loss, the prediction is score before logistic transformation\n",
    "# Keep this in mind when you use the customization\n",
    "def binary_error(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    preds = 1. / (1. + np.exp(-preds))\n",
    "    return 'error', np.mean(labels != (preds > 0.5)), False\n",
    "\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10,\n",
    "                init_model=gbm,\n",
    "                fobj=loglikelihood,\n",
    "                feval=binary_error,\n",
    "                valid_sets=lgb_eval)\n",
    "\n",
    "print('Finished 40 - 50 rounds with self-defined objective function and eval metric...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81]\tvalid_0's binary_logloss: 5.60831\tvalid_0's accuracy: 0.742\n",
      "[82]\tvalid_0's binary_logloss: 5.60753\tvalid_0's accuracy: 0.742\n",
      "[83]\tvalid_0's binary_logloss: 5.60459\tvalid_0's accuracy: 0.742\n",
      "[84]\tvalid_0's binary_logloss: 5.7262\tvalid_0's accuracy: 0.736\n",
      "[85]\tvalid_0's binary_logloss: 5.66634\tvalid_0's accuracy: 0.736\n",
      "[86]\tvalid_0's binary_logloss: 5.66577\tvalid_0's accuracy: 0.736\n",
      "[87]\tvalid_0's binary_logloss: 5.72674\tvalid_0's accuracy: 0.738\n",
      "[88]\tvalid_0's binary_logloss: 5.72827\tvalid_0's accuracy: 0.738\n",
      "[89]\tvalid_0's binary_logloss: 5.73348\tvalid_0's accuracy: 0.738\n",
      "[90]\tvalid_0's binary_logloss: 5.72232\tvalid_0's accuracy: 0.74\n",
      "Finished 50 - 60 rounds with self-defined objective function and multiple self-defined eval metrics...\n",
      "Starting a new training job...\n",
      "[1]\ttraining's binary_logloss: 0.455912\n",
      "[2]\ttraining's binary_logloss: 0.454606\n",
      "[3]\ttraining's binary_logloss: 0.453423\n",
      "[4]\ttraining's binary_logloss: 0.452032\n",
      "[5]\ttraining's binary_logloss: 0.450779\n",
      "Add a new valid dataset at iteration 5...\n",
      "[6]\ttraining's binary_logloss: 0.449584\tnew_valid's binary_logloss: 0.689177\n",
      "[7]\ttraining's binary_logloss: 0.448249\tnew_valid's binary_logloss: 0.688093\n",
      "[8]\ttraining's binary_logloss: 0.447108\tnew_valid's binary_logloss: 0.687629\n",
      "[9]\ttraining's binary_logloss: 0.446104\tnew_valid's binary_logloss: 0.686687\n",
      "[10]\ttraining's binary_logloss: 0.444945\tnew_valid's binary_logloss: 0.685686\n",
      "Finished first 10 rounds with callback function...\n"
     ]
    }
   ],
   "source": [
    "# another self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool\n",
    "# accuracy\n",
    "# NOTE: when you do customized loss function, the default prediction value is margin\n",
    "# This may make built-in evalution metric calculate wrong results\n",
    "# For example, we are doing log likelihood loss, the prediction is score before logistic transformation\n",
    "# Keep this in mind when you use the customization\n",
    "def accuracy(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    preds = 1. / (1. + np.exp(-preds))\n",
    "    return 'accuracy', np.mean(labels == (preds > 0.5)), True\n",
    "\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10,\n",
    "                init_model=gbm,\n",
    "                fobj=loglikelihood,\n",
    "                feval= accuracy,\n",
    "                valid_sets=lgb_eval)\n",
    "\n",
    "print('Finished 50 - 60 rounds with self-defined objective function '\n",
    "      'and multiple self-defined eval metrics...')\n",
    "\n",
    "print('Starting a new training job...')\n",
    "\n",
    "\n",
    "# callback\n",
    "def reset_metrics():\n",
    "    def callback(env):\n",
    "        lgb_eval_new = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "        if env.iteration - env.begin_iteration == 5:\n",
    "            print('Add a new valid dataset at iteration 5...')\n",
    "            env.model.add_valid(lgb_eval_new, 'new_valid')\n",
    "    callback.before_iteration = True\n",
    "    callback.order = 0\n",
    "    return callback\n",
    "\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10,\n",
    "                valid_sets=lgb_train,\n",
    "                callbacks=[reset_metrics()])\n",
    "\n",
    "print('Finished first 10 rounds with callback function...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
